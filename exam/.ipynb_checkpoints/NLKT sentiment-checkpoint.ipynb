{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Anders\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Anders\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Anders\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Anders\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Anders\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import packages\n",
    "import nltk, re, string, random, tqdm\n",
    "import pandas as pd\n",
    "from nltk import classify, NaiveBayesClassifier, FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Raw training data\n",
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "text = twitter_samples.strings('tweets.20150430-223406.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lematization funtion, normalises words to canonical form\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to remove noise (the, it, @ for twitter handles and others)\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make clean sample out of cleaned data\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_tokens_list = []\n",
    "negative_cleaned_tokens_list = []\n",
    "\n",
    "for tokens in positive_tweet_tokens:\n",
    "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "for tokens in negative_tweet_tokens:\n",
    "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word density function\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "#print(freq_dist_pos.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency distribution\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "#print(freq_dist_pos.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to fetch cleaned tokens for modelling\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataset\n",
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.9963333333333333\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2071.9 : 1.0\n",
      "                      :) = True           Positi : Negati =    982.3 : 1.0\n",
      "                follower = True           Positi : Negati =     32.9 : 1.0\n",
      "                     sad = True           Negati : Positi =     22.2 : 1.0\n",
      "                     x15 = True           Negati : Positi =     17.7 : 1.0\n",
      "                     via = True           Positi : Negati =     15.2 : 1.0\n",
      "              appreciate = True           Positi : Negati =     14.9 : 1.0\n",
      "                    glad = True           Positi : Negati =     13.3 : 1.0\n",
      "                  arrive = True           Positi : Negati =     11.8 : 1.0\n",
      "                     idk = True           Negati : Positi =     11.0 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#classifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom tweet tester, model seems to not understand sarcasm\n",
    "custom_tweet = 'autism'\n",
    "\n",
    "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 884949/884949 [21:54<00:00, 673.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884944</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884945</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884946</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884947</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884948</th>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>884949 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0\n",
       "0       1\n",
       "1      -1\n",
       "2       1\n",
       "3      -1\n",
       "4      -1\n",
       "...    ..\n",
       "884944 -1\n",
       "884945  1\n",
       "884946  1\n",
       "884947 -1\n",
       "884948 -1\n",
       "\n",
       "[884949 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Analyse my Data\n",
    "File = 'reddit-submissions-dogecoin.csv'\n",
    "dogecoin_df = pd.read_csv(File, encoding='utf-8')\n",
    "output_list = []\n",
    "temp_list = dogecoin_df['Title'].tolist()\n",
    "for i in tqdm.tqdm(temp_list):\n",
    "    custom_text = str(i)\n",
    "    custom_tokens = remove_noise(word_tokenize(custom_text))\n",
    "    if classifier.classify(dict([token, True] for token in custom_tokens)) == 'Negative':\n",
    "        output_list.append(-1)\n",
    "    else:\n",
    "        output_list.append(1)\n",
    "output_df = pd.DataFrame(output_list)\n",
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>Publish Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Updated 2020] Best Dogecoin Faucets — how to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-09-01 01:04:31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How much dogecoin you people have?</td>\n",
       "      <td>-1</td>\n",
       "      <td>2020-09-01 01:08:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Do I HODL?</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-09-01 01:13:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How much dogecoin you people have?</td>\n",
       "      <td>-1</td>\n",
       "      <td>2020-09-01 01:24:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Take my money</td>\n",
       "      <td>-1</td>\n",
       "      <td>2020-09-01 02:56:21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884944</th>\n",
       "      <td>Doge Money!!!</td>\n",
       "      <td>-1</td>\n",
       "      <td>2021-12-30 23:11:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884945</th>\n",
       "      <td>Let the Wizard of Doge guide your fate!</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-12-30 23:14:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884946</th>\n",
       "      <td>Most viewed topic on reddit in 2021 - Crypto -...</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-12-30 23:41:50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884947</th>\n",
       "      <td>.17 floor? No .16 floor? No, there are no floo...</td>\n",
       "      <td>-1</td>\n",
       "      <td>2021-12-30 23:47:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884948</th>\n",
       "      <td>Do only good everyday</td>\n",
       "      <td>-1</td>\n",
       "      <td>2021-12-30 23:49:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>884949 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Title  sentiment  \\\n",
       "0       [Updated 2020] Best Dogecoin Faucets — how to ...          1   \n",
       "1                      How much dogecoin you people have?         -1   \n",
       "2                                              Do I HODL?          1   \n",
       "3                      How much dogecoin you people have?         -1   \n",
       "4                                           Take my money         -1   \n",
       "...                                                   ...        ...   \n",
       "884944                                      Doge Money!!!         -1   \n",
       "884945            Let the Wizard of Doge guide your fate!          1   \n",
       "884946  Most viewed topic on reddit in 2021 - Crypto -...          1   \n",
       "884947  .17 floor? No .16 floor? No, there are no floo...         -1   \n",
       "884948                              Do only good everyday         -1   \n",
       "\n",
       "               Publish Date  \n",
       "0       2020-09-01 01:04:31  \n",
       "1       2020-09-01 01:08:29  \n",
       "2       2020-09-01 01:13:10  \n",
       "3       2020-09-01 01:24:38  \n",
       "4       2020-09-01 02:56:21  \n",
       "...                     ...  \n",
       "884944  2021-12-30 23:11:44  \n",
       "884945  2021-12-30 23:14:12  \n",
       "884946  2021-12-30 23:41:50  \n",
       "884947  2021-12-30 23:47:35  \n",
       "884948  2021-12-30 23:49:17  \n",
       "\n",
       "[884949 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dogecoin_df['sentiment'] = output_df\n",
    "\n",
    "dogecoin_df[['Title', 'sentiment', 'Publish Date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dogecoin_senti_df = dogecoin_df[['Title', 'sentiment', 'Publish Date']]\n",
    "dogecoin_senti_df.to_csv('dogecoin_sentimental.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dogecoin_senti_df['Publish Date'].astype(str).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
